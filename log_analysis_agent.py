import os
import time
import uuid
from click import prompt
from fastapi import FastAPI, HTTPException, Request   # ‚úÖ FIXED
from fastapi.responses import Response
from pydantic import BaseModel
from elasticsearch import Elasticsearch
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langfuse import Langfuse
from dotenv import load_dotenv
from openai import OpenAI
import urllib3

# ------------------ Observability Imports ------------------
from opentelemetry import trace
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# ------------------ Load environment variables ------------------
load_dotenv()
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ------------------ FastAPI App ------------------
app = FastAPI(title="RCA Agent", version="1.0.0")

# Initialize OpenAI client
client = OpenAI(
    base_url=os.getenv("MAJORDOMO_AI_GATEWAY_URL"),
    api_key=os.getenv("MAJORDOMO_AI_API_KEY"),
)

# ------------------ OpenTelemetry Setup ------------------
resource = Resource.create({
    "service.name": "rca-agent",
    "service.namespace": "log-analysis",
    "service.version": "1.0.0",
})
provider = TracerProvider(resource=resource)
processor = BatchSpanProcessor(
    OTLPSpanExporter(endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT"), insecure=True)
)
print("üîå Connecting to OpenTelemetry Collector at", os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT"))
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
# tracer = trace.get_tracer("log-analyzer-agent")

# ------------------ Initialize Clients ------------------
es = Elasticsearch(
    os.getenv("ELASTICSEARCH_URL"), 
    verify_certs=False,
    headers={"Accept": "application/vnd.elasticsearch+json;compatible-with=8"}
)
print("üîå Connecting to Elasticsearch at", os.getenv("ELASTICSEARCH_URL"))
es._otel.enabled = False  # Disable OpenTelemetry instrumentation for Elasticsearch client

# Retry connection with backoff
max_retries = 5
for attempt in range(max_retries):
    try:
        if es.ping():
            print("‚úÖ Successfully connected to Elasticsearch")
            break
    except Exception as e:
        print(f"‚ùå Attempt {attempt + 1}/{max_retries} failed to connect to Elasticsearch: {e}")
        if attempt < max_retries - 1:
            time.sleep(5 * (attempt + 1))  # Exponential backoff
        else:
            print("üö® Failed to connect to Elasticsearch after all retries")
            raise Exception("Cannot connect to Elasticsearch")

# openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
langfuse = Langfuse(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_HOST")
)

# ------------------ Instrument FastAPI (Disabled - too verbose) ------------------
# try:
#     from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
#     FastAPIInstrumentor.instrument_app(app)
#     print("‚úÖ FastAPI instrumentation enabled")
# except ImportError as e:
#     print(f"‚ö†Ô∏è FastAPI instrumentation not available: {e}")

# ------------------ Agent State ------------------
class State(dict):
    logs: str
    summary: str
    part1_summary: str = None
    part2_summary: str = None
    main_trace_id: str = None

workflow_trace = None

# ------------------ Agent Steps ------------------
def fetch_logs(state: State) -> State:
    global workflow_trace
    print("üîç Fetching latest logs from Elasticsearch...")
    start_time = time.time()
    fetch_obs = workflow_trace.start_observation(
        name="fetch_logs", as_type="span",
        input={"elasticsearch_index": "k8s-logs", "query_size": 10},
        metadata={"function": "fetch_logs", "start_time": start_time}
    )
    try:
        response = es.search(
            index="k8s-logs",
            body={"size": 10, "sort": [{"timestamp": {"order": "desc"}}], "query": {"match_all": {}}}
        )
        logs_str = "Kubernetes Logs:\n\n"
        for i, hit in enumerate(response['hits']['hits'], 1):
            src = hit['_source']
            logs_str += f"Log {i}:\nTimestamp: {src.get('timestamp')}\nLevel: {src.get('level')}\nMessage: {src.get('message')}\n{'-'*40}\n"
        
        logs_count = len(response['hits']['hits'])
        fetch_obs.update(output={
            "logs_retrieved": logs_count,
            "logs_preview": logs_str[:500] + "..." if len(logs_str) > 500 else logs_str
        })
        fetch_obs.end()
        state["logs"] = logs_str
    except Exception as e:
        fetch_obs.update(output={"error": str(e)}, metadata={"status": "error"})
        fetch_obs.end()
        raise e
    return state


def analyze_logs(state: State) -> State:
    global workflow_trace
    print("üß† Analyzing logs with split processing...")
    start_time = time.time()
    
    # Create parent observation for the entire analysis process
    analyze_obs = workflow_trace.start_observation(
        name="analyze_logs_openai", as_type="span",
        input={"logs_length": len(state['logs']), "task": "split_and_analyze_logs"},
        metadata={"function": "analyze_logs", "start_time": start_time, "processing_method": "split_analysis"}
    )

    logs = state.get('logs', '')
    MAJORDOMO_AI_MODEL = os.getenv("MAJORDOMO_AI_MODEL")
    
    # Split logs into two parts
    log_lines = logs.split('\n')
    mid_point = len(log_lines) // 2
    
    logs_part1 = '\n'.join(log_lines[:mid_point])
    logs_part2 = '\n'.join(log_lines[mid_point:])
    
    print(f"üìä Split logs into 2 parts: Part 1 ({len(logs_part1)} chars), Part 2 ({len(logs_part2)} chars)")
    
    # Get the analysis prompt template
    prompt_template = langfuse.get_prompt("log_analysis_prompt", label="production")
    
    # === FIRST OpenAI CALL - Analyze Part 1 ===
    print("üîç Analyzing logs part 1...")
    messages_part1 = prompt_template.compile(logs=logs_part1)
    
    openai_obs_1 = analyze_obs.start_observation(
        name="openai_analysis_part1", as_type="generation",
        model=MAJORDOMO_AI_MODEL, 
        input={
            "messages": messages_part1,
            "part": "1_of_2",
            "logs_length": len(logs_part1)
        },
        metadata={"log_part": "first_half", "analysis_step": "1"}
    )

    response_1 = client.chat.completions.create(
        model=MAJORDOMO_AI_MODEL, 
        messages=messages_part1,
        temperature=1.0,
        max_tokens=512,
        extra_headers={"X-Trace-ID": state.get("main_trace_id")}
    )

    summary_part1 = response_1.choices[0].message.content
    openai_obs_1.update(output={
        "summary": summary_part1,
        "tokens_used": response_1.usage.total_tokens if response_1.usage else None
    })
    openai_obs_1.end()
    print(f"‚úÖ Part 1 analysis complete ({len(summary_part1)} chars)")

    # === SECOND OpenAI CALL - Analyze Part 2 ===
    print("üîç Analyzing logs part 2...")
    messages_part2 = prompt_template.compile(logs=logs_part2)
    
    openai_obs_2 = analyze_obs.start_observation(
        name="openai_analysis_part2", as_type="generation",
        model=MAJORDOMO_AI_MODEL, 
        input={
            "messages": messages_part2,
            "part": "2_of_2",
            "logs_length": len(logs_part2)
        },
        metadata={"log_part": "second_half", "analysis_step": "2"}
    )

    response_2 = client.chat.completions.create(
        model=MAJORDOMO_AI_MODEL, 
        messages=messages_part2,
        temperature=1.0,
        max_tokens=512,
        extra_headers={"X-Trace-ID": state.get("main_trace_id")}
    )

    summary_part2 = response_2.choices[0].message.content
    openai_obs_2.update(output={
        "summary": summary_part2,
        "tokens_used": response_2.usage.total_tokens if response_2.usage else None
    })
    openai_obs_2.end()
    print(f"‚úÖ Part 2 analysis complete ({len(summary_part2)} chars)")

    # === THIRD OpenAI CALL - Combine Summaries ===
    print("üîó Combining summaries...")
    combine_messages = [
        {
            "role": "system",
            "content": "You are a log analysis expert. Combine the following two log analysis summaries into one comprehensive summary. Identify common patterns, prioritize the most critical issues, and provide actionable insights."
        },
        {
            "role": "user",
            "content": f"""Please combine these two log analysis summaries into one comprehensive summary:

PART 1 ANALYSIS:
{summary_part1}

PART 2 ANALYSIS:
{summary_part2}

Provide a unified summary that:
1. Highlights the most critical issues across both parts
2. Identifies patterns or connections between the parts
3. Prioritizes issues by severity
4. Provides actionable recommendations"""
        }
    ]
    
    openai_obs_combine = analyze_obs.start_observation(
        name="openai_combine_summaries", as_type="generation",
        model=MAJORDOMO_AI_MODEL, 
        input={
            "messages": combine_messages,
            "task": "combine_summaries",
            "part1_length": len(summary_part1),
            "part2_length": len(summary_part2)
        },
        metadata={"analysis_step": "3_combine", "summary_type": "final_unified"}
    )

    response_combined = client.chat.completions.create(
        model=MAJORDOMO_AI_MODEL, 
        messages=combine_messages,
        temperature=0.7,  # Lower temperature for more consistent combining
        max_tokens=1024,  # More tokens for comprehensive summary
        extra_headers={"X-Trace-ID": state.get("main_trace_id")}
    )

    final_summary = response_combined.choices[0].message.content
    openai_obs_combine.update(output={
        "final_summary": final_summary,
        "tokens_used": response_combined.usage.total_tokens if response_combined.usage else None
    })
    openai_obs_combine.end()
    print(f"‚úÖ Final combined summary complete ({len(final_summary)} chars)")

    # Update parent observation with complete results
    execution_time = time.time() - start_time
    analyze_obs.update(output={
        "analysis_completed": True,
        "processing_method": "split_analysis",
        "parts_processed": 2,
        "summaries": {
            "part1_summary": summary_part1,
            "part2_summary": summary_part2,
            "final_combined_summary": final_summary
        },
        "execution_metrics": {
            "total_execution_time": execution_time,
            "openai_calls_made": 3,
            "logs_part1_chars": len(logs_part1),
            "logs_part2_chars": len(logs_part2),
            "final_summary_chars": len(final_summary)
        }
    })
    analyze_obs.end()

    # Store the final combined summary in state
    state["summary"] = final_summary
    state["part1_summary"] = summary_part1
    state["part2_summary"] = summary_part2
    
    return state


def display_summary(state: State) -> State:
    global workflow_trace
    print("‚úÖ Log Analysis Results:\n")
    
    # Get all summaries
    final_summary = state.get("summary", "No final summary available")
    part1_summary = state.get("part1_summary", "No part 1 summary available")
    part2_summary = state.get("part2_summary", "No part 2 summary available")
    
    obs = workflow_trace.start_observation(
        name="display_summary", as_type="span",
        input={
            "final_summary_length": len(final_summary),
            "part1_summary_length": len(part1_summary),
            "part2_summary_length": len(part2_summary),
            "display_target": "console"
        },
        metadata={
            "function": "display_summary", 
            "output_method": "print",
            "analysis_method": "split_processing"
        }
    )
    
    # Display the results
    print("=" * 80)
    print("üîç PART 1 ANALYSIS:")
    print("-" * 40)
    print(part1_summary)
    print("\n")
    
    print("üîç PART 2 ANALYSIS:")
    print("-" * 40)
    print(part2_summary)
    print("\n")
    
    print("üéØ FINAL COMBINED SUMMARY:")
    print("-" * 40)
    print(final_summary)
    print("=" * 80)
    
    # Update observation with output details
    obs.update(output={
        "summaries_displayed": {
            "part1_displayed": True,
            "part2_displayed": True, 
            "final_summary_displayed": True
        },
        "display_method": "console_print",
        "total_characters_displayed": len(part1_summary) + len(part2_summary) + len(final_summary),
        "display_sections": 3
    })
    obs.end()
    return state

# ------------------ Workflow Graph ------------------
workflow = StateGraph(State)
workflow.add_node("fetch_logs", fetch_logs)
workflow.add_node("analyze_logs", analyze_logs)
workflow.add_node("display_summary", display_summary)

workflow.add_edge(START, "fetch_logs")
workflow.add_edge("fetch_logs", "analyze_logs")
workflow.add_edge("analyze_logs", "display_summary")
workflow.add_edge("display_summary", END)

# Compile workflow once
memory = MemorySaver()
app_graph = workflow.compile(checkpointer=memory)

# ------------------ API Schema ------------------
class WorkflowRequest(BaseModel):
    index: str = "k8s-logs"

# ------------------ API Endpoint ------------------
@app.post("/run-workflow")
async def run_workflow(request: WorkflowRequest):
    """Trigger the log analysis workflow."""
    try:
        global workflow_trace
        workflow_start = time.time()

        # Create Langfuse observation for HTTP request
        http_obs = langfuse.start_observation(
            name="POST /run-workflow",
            as_type="span",
            input={
                "http_method": "POST",
                "endpoint": "/run-workflow",
                "request_body": request.dict(),
                "content_type": "application/json",
                "timestamp": workflow_start
            },
            metadata={
                "http_route": "/run-workflow",
                "service": "log-analyzer-agent",
                "request_type": "api_call"
            }
        )

        # Create Langfuse trace for workflow (child of HTTP request)
        workflow_trace = http_obs.start_observation(
            name="rca_agent_workflow",
            as_type="trace",
            input={
                "workflow_type": "elasticsearch_log_analysis", 
                "elasticsearch_index": request.index,
                "query_parameters": {
                    "size": 10,
                    "sort": [{"timestamp": {"order": "desc"}}],
                    "query": {"match_all": {}}
                },
                "timestamp": workflow_start,
                "user_request": request.dict()
            },
            metadata={
                "start_time": workflow_start, 
                "elasticsearch_index": request.index,
                "service": "log-analyzer-agent"
            }
        )

        # Run workflow
        final_state = app_graph.invoke(
            {"logs": "", "main_trace_id": workflow_trace.trace_id},
            config={"stream": False, "configurable": {"thread_id": "log_analysis_agent"}}
        )

        # Calculate execution metrics
        execution_time = time.time() - workflow_start
        summary = final_state.get("summary", "No summary available")
        logs_processed = len(final_state.get("logs", ""))

        # Update workflow trace
        workflow_trace.update(
            output={
                "workflow_completed": True,
                "execution_metrics": {
                    "total_execution_time_seconds": execution_time,
                    "logs_processed_chars": logs_processed,
                    "summary_length_chars": len(summary)
                },
                "results": {
                    "final_summary": summary,
                    "logs_found": logs_processed > 0,
                    "analysis_status": "completed"
                }
            },
            metadata={
                "status": "completed",
                "end_time": time.time(),
                "execution_duration": execution_time
            }
        )
        workflow_trace.end()

        # Prepare API response
        api_response = {
            "status": "success",
            "trace_id": workflow_trace.trace_id,
            "summary": summary,
            "execution_time": execution_time,
            "analysis_method": "split_processing",
            "details": {
                "part1_summary": final_state.get("part1_summary"),
                "part2_summary": final_state.get("part2_summary"),
                "final_combined_summary": summary,
                "openai_calls_made": 3
            }
        }

        # Update HTTP observation with output
        http_obs.update(
            output={
                "http_status_code": 200,
                "response_body": api_response,
                "execution_time_seconds": execution_time,
                "workflow_trace_id": workflow_trace.trace_id,
                "content_type": "application/json"
            },
            metadata={
                "status": "completed",
                "response_size_chars": len(str(api_response))
            }
        )
        http_obs.end()
        langfuse.flush()

        return api_response

    except Exception as e:
        # Prepare error response
        error_response = {"status": "error", "error": str(e)}
        
        # Update HTTP observation on error
        if 'http_obs' in locals():
            http_obs.update(
                output={
                    "http_status_code": 500,
                    "response_body": error_response,
                    "error_details": {
                        "error_message": str(e),
                        "error_type": type(e).__name__
                    }
                },
                metadata={"status": "error", "error": str(e)}
            )
            http_obs.end()
        
        # Update workflow trace on error
        if 'workflow_trace' in locals():
            workflow_trace.update(
                output={
                    "workflow_completed": False,
                    "error_details": {
                        "error_message": str(e),
                        "error_type": type(e).__name__,
                        "execution_time_before_error": time.time() - workflow_start if 'workflow_start' in locals() else 0
                    }
                },
                metadata={
                    "status": "error", 
                    "error": str(e),
                    "end_time": time.time()
                }
            )
            workflow_trace.end()
        
        langfuse.flush()
        raise HTTPException(status_code=500, detail=str(e))
    
    
    
    
# the traces how flow the data to collector
# languages jaeger 
# state as input output